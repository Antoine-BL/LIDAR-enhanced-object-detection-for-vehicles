{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from descartes import PolygonPatch\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import alphashape\n",
    "import cv2\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import open3d as o3d\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.utils import range_image_utils\n",
    "from waymo_open_dataset.utils import transform_utils\n",
    "from waymo_open_dataset.utils import  frame_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/lidar tests/training/' + file for file in os.listdir('Data/lidar tests/training')]\n",
    "dataset = tf.data.TFRecordDataset(files, buffer_size=tf.constant(int(pow(10,6)), tf.int64), num_parallel_reads=16)\n",
    "OUTPUT_PATH = 'Data/darknet-data'\n",
    "SCALE_PCT = 0.5\n",
    "img_id = len(os.listdir(f'{OUTPUT_PATH}/obj')) // 2\n",
    "\n",
    "def label_in_polygon(label, image):\n",
    "    top_left_x = int(math.ceil(label.box.center_x - label.box.length / 2))\n",
    "    top_left_y = int(math.ceil(label.box.center_y - label.box.width / 2))\n",
    "    bottom_right_x = int(math.ceil(label.box.center_x + label.box.length / 2))\n",
    "    bottom_right_y = int(math.ceil(label.box.center_y + label.box.width / 2))\n",
    "    cropped = image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "    nb_pixels = cropped.size\n",
    "    nonzero = np.count_nonzero(cropped)\n",
    "    return (nonzero / nb_pixels) > 0.25\n",
    "\n",
    "training_index = open('Data/darknet-data/training.txt', 'w')\n",
    "for data in dataset:\n",
    "    frame = open_dataset.Frame()\n",
    "    frame.ParseFromString(bytearray(data.numpy()))\n",
    "    image = frame.images[0]\n",
    "    image = np.array(tf.io.decode_jpeg(image.image))\n",
    "\n",
    "    if np.mean(image) < 100:\n",
    "        continue\n",
    "\n",
    "    (range_images, camera_projections, _, range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
    "    points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
    "    frame,\n",
    "    range_images,\n",
    "    camera_projections,\n",
    "    range_image_top_pose)\n",
    "    points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
    "        frame,\n",
    "        range_images,\n",
    "        camera_projections,\n",
    "        range_image_top_pose,\n",
    "        ri_index=1)\n",
    "    # 3d points in vehicle frame.\n",
    "    points_all = np.concatenate(points, axis=0)\n",
    "    points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
    "    # camera projection corresponding to each point.\n",
    "    cp_points_all = np.concatenate(cp_points, axis=0)\n",
    "    cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)\n",
    "\n",
    "    all_points = np.append(points_all, points_all_ri2, axis=0)\n",
    "    all_points_cp = np.append(cp_points_all, cp_points_all_ri2, axis=0)\n",
    "\n",
    "    front_points = np.array([point for (point, cp) in zip(all_points, all_points_cp) if cp[0] == 1 or cp[3] == 1])\n",
    "    front_cp_points = np.array([cp[:3] if cp[0] == 1 else cp[3:] for cp in all_points_cp if cp[0] == 1 or cp[3] == 1])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(front_points)\n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold=0.1, ransac_n=4, num_iterations=1000)\n",
    "\n",
    "    front_cp_points_no_road = np.delete(front_cp_points, inliers, axis=0)\n",
    "\n",
    "    alpha_shape = alphashape.alphashape(front_cp_points_no_road[:,1:3], 0.02)\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "\n",
    "    try:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in alpha_shape]\n",
    "    except:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in [alpha_shape]]\n",
    "\n",
    "    mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "    cv2.fillPoly(mask, exteriors, 1)\n",
    "    image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    labels = [camera_labels.labels for camera_labels in frame.camera_labels if camera_labels.name == 1][0]\n",
    "    labels = [label for label in labels if label.box.length > 50 and label.box.length > 50 and label_in_polygon(label, image)]\n",
    "\n",
    "    og_width = image.shape[1]\n",
    "    og_height = image.shape[0]\n",
    "    img_width = 800\n",
    "    img_height = 533\n",
    "    image = Image.fromarray(image)    \n",
    "    image = image.resize((img_width, img_height))\n",
    "\n",
    "    image_name = f'example-{img_id}.jpg'\n",
    "    image.save(f'{OUTPUT_PATH}/obj/{image_name}')\n",
    "\n",
    "    label_name = f'example-{img_id}.txt'\n",
    "    with open(f'{OUTPUT_PATH}/obj/{label_name}', 'w') as label_file:\n",
    "        for label in labels:\n",
    "            cx = label.box.center_x \n",
    "            cy = label.box.center_y\n",
    "            w = label.box.length\n",
    "            h = label.box.width\n",
    "            cx /= og_width\n",
    "            cy /= og_height\n",
    "            w /= og_width\n",
    "            h /= og_height\n",
    "            clazz = label.type\n",
    "            darknet_labels = f'{clazz} {cx} {cy} {w} {h}\\n'\n",
    "            label_file.write(darknet_labels)\n",
    "\n",
    "    training_index.write(f'data/obj/{image_name}\\n')\n",
    "    img_id += 1\n",
    "\n",
    "training_index.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/lidar tests/validation/' + file for file in os.listdir('Data/lidar tests/validation')]\n",
    "\n",
    "validation_index = open('Data/darknet-data/validation.txt', 'w')\n",
    "img_id = len(os.listdir(f'{OUTPUT_PATH}/obj')) // 2\n",
    "def label_in_polygon(label, image):\n",
    "    top_left_x = int(math.ceil(label.box.center_x - label.box.length / 2))\n",
    "    top_left_y = int(math.ceil(label.box.center_y - label.box.width / 2))\n",
    "    bottom_right_x = int(math.ceil(label.box.center_x + label.box.length / 2))\n",
    "    bottom_right_y = int(math.ceil(label.box.center_y + label.box.width / 2))\n",
    "    cropped = image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "    nb_pixels = cropped.size\n",
    "    nonzero = np.count_nonzero(cropped)\n",
    "    return (nonzero / nb_pixels) > 0.25\n",
    "\n",
    "\n",
    "for data in dataset:\n",
    "    frame = open_dataset.Frame()\n",
    "    frame.ParseFromString(bytearray(data.numpy()))\n",
    "    image = frame.images[0]\n",
    "    image = np.array(tf.io.decode_jpeg(image.image))\n",
    "\n",
    "    if np.mean(image) < 100:\n",
    "        continue\n",
    "\n",
    "    (range_images, camera_projections, _, range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
    "    points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
    "    frame,\n",
    "    range_images,\n",
    "    camera_projections,\n",
    "    range_image_top_pose)\n",
    "    points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
    "        frame,\n",
    "        range_images,\n",
    "        camera_projections,\n",
    "        range_image_top_pose,\n",
    "        ri_index=1)\n",
    "    # 3d points in vehicle frame.\n",
    "    points_all = np.concatenate(points, axis=0)\n",
    "    points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
    "    # camera projection corresponding to each point.\n",
    "    cp_points_all = np.concatenate(cp_points, axis=0)\n",
    "    cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)\n",
    "\n",
    "    all_points = np.append(points_all, points_all_ri2, axis=0)\n",
    "    all_points_cp = np.append(cp_points_all, cp_points_all_ri2, axis=0)\n",
    "\n",
    "    front_points = np.array([point for (point, cp) in zip(all_points, all_points_cp) if cp[0] == 1 or cp[3] == 1])\n",
    "    front_cp_points = np.array([cp[:3] if cp[0] == 1 else cp[3:] for cp in all_points_cp if cp[0] == 1 or cp[3] == 1])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(front_points)\n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold=0.1, ransac_n=4, num_iterations=1000)\n",
    "\n",
    "    front_cp_points_no_road = np.delete(front_cp_points, inliers, axis=0)\n",
    "\n",
    "    alpha_shape = alphashape.alphashape(front_cp_points_no_road[:,1:3], 0.02)\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "\n",
    "    try:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in alpha_shape]\n",
    "    except:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in [alpha_shape]]\n",
    "\n",
    "    mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "    cv2.fillPoly(mask, exteriors, 1)\n",
    "    image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    labels = [camera_labels.labels for camera_labels in frame.camera_labels if camera_labels.name == 1][0]\n",
    "    labels = [label for label in labels if label.box.length > 50 and label.box.length > 50]\n",
    "\n",
    "    og_width = image.shape[1]\n",
    "    og_height = image.shape[0]\n",
    "    img_width = 800\n",
    "    img_height = 533\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((img_width, img_height))\n",
    "\n",
    "    image_name = f'example-{img_id}.jpg'\n",
    "    image.save(f'{OUTPUT_PATH}/obj/{image_name}')\n",
    "\n",
    "    label_name = f'example-{img_id}.txt'\n",
    "    with open(f'{OUTPUT_PATH}/obj/{label_name}', 'w') as label_file:\n",
    "        for label in labels:\n",
    "            cx = label.box.center_x \n",
    "            cy = label.box.center_y\n",
    "            w = label.box.length\n",
    "            h = label.box.width\n",
    "            cx /= og_width\n",
    "            cy /= og_height\n",
    "            w /= og_width\n",
    "            h /= og_height\n",
    "            clazz = label.type\n",
    "            darknet_labels = f'{clazz} {cx} {cy} {w} {h}\\n'\n",
    "            label_file.write(darknet_labels)\n",
    "\n",
    "    validation_index.write(f'data/obj/{image_name}\\n')\n",
    "    img_id += 1\n",
    "\n",
    "validation_index.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5277/9818701.py:67: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  exteriors = [int_coords(poly.exterior.coords) for poly in alpha_shape]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=29'>30</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=31'>32</a>\u001b[0m (range_images, camera_projections, _, range_image_top_pose) \u001b[39m=\u001b[39m frame_utils\u001b[39m.\u001b[39mparse_range_image_and_camera_projection(frame)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=32'>33</a>\u001b[0m points, cp_points \u001b[39m=\u001b[39m frame_utils\u001b[39m.\u001b[39;49mconvert_range_image_to_point_cloud(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=33'>34</a>\u001b[0m frame,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=34'>35</a>\u001b[0m range_images,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=35'>36</a>\u001b[0m camera_projections,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=36'>37</a>\u001b[0m range_image_top_pose)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=37'>38</a>\u001b[0m points_ri2, cp_points_ri2 \u001b[39m=\u001b[39m frame_utils\u001b[39m.\u001b[39mconvert_range_image_to_point_cloud(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=38'>39</a>\u001b[0m     frame,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=39'>40</a>\u001b[0m     range_images,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=40'>41</a>\u001b[0m     camera_projections,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=41'>42</a>\u001b[0m     range_image_top_pose,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=42'>43</a>\u001b[0m     ri_index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/antoinebrassardlahey/MTI830/lidar_pretreat.ipynb#ch0000004?line=43'>44</a>\u001b[0m \u001b[39m# 3d points in vehicle frame.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/waymo_open_dataset/utils/frame_utils.py:215\u001b[0m, in \u001b[0;36mconvert_range_image_to_point_cloud\u001b[0;34m(frame, range_images, camera_projections, range_image_top_pose, ri_index, keep_polar_features)\u001b[0m\n\u001b[1;32m    212\u001b[0m points \u001b[39m=\u001b[39m []\n\u001b[1;32m    213\u001b[0m cp_points \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 215\u001b[0m cartesian_range_images \u001b[39m=\u001b[39m convert_range_image_to_cartesian(\n\u001b[1;32m    216\u001b[0m     frame, range_images, range_image_top_pose, ri_index, keep_polar_features)\n\u001b[1;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m calibrations:\n\u001b[1;32m    219\u001b[0m   range_image \u001b[39m=\u001b[39m range_images[c\u001b[39m.\u001b[39mname][ri_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/waymo_open_dataset/utils/frame_utils.py:156\u001b[0m, in \u001b[0;36mconvert_range_image_to_cartesian\u001b[0;34m(frame, range_images, range_image_top_pose, ri_index, keep_polar_features)\u001b[0m\n\u001b[1;32m    152\u001b[0m beam_inclinations \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreverse(beam_inclinations, axis\u001b[39m=\u001b[39m[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    153\u001b[0m extrinsic \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39marray(c\u001b[39m.\u001b[39mextrinsic\u001b[39m.\u001b[39mtransform), [\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m])\n\u001b[1;32m    155\u001b[0m range_image_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(\n\u001b[0;32m--> 156\u001b[0m     tf\u001b[39m.\u001b[39;49mconvert_to_tensor(value\u001b[39m=\u001b[39;49mrange_image\u001b[39m.\u001b[39;49mdata), range_image\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mdims)\n\u001b[1;32m    157\u001b[0m pixel_pose_local \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    158\u001b[0m frame_pose_local \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1430\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   1367\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1369\u001b[0m     value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1370\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \n\u001b[1;32m   1372\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1430\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[1;32m   1431\u001b[0m       value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1436\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1435\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1436\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor(\n\u001b[1;32m   1437\u001b[0m       value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m   1438\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1439\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1440\u001b[0m       preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint,\n\u001b[1;32m   1441\u001b[0m       as_ref\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1561\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1563\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   1565\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1566\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1568\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1569\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:346\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    344\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    345\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 346\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    176\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    272\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:283\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 283\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    285\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    286\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:308\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    307\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    309\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/mti830-3d/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    105\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files = ['Data/lidar tests/validation/' + file for file in os.listdir('Data/lidar tests/validation')]\n",
    "\n",
    "nb_frames = 0\n",
    "nb_labels = 0\n",
    "nb_frames_too_dark = 0\n",
    "nb_labels_too_small = 0\n",
    "nb_labels_not_in_poly = 0\n",
    "labels_per_class = [0] * 5\n",
    "\n",
    "def label_in_polygon(label, image):\n",
    "    top_left_x = int(math.ceil(label.box.center_x - label.box.length / 2))\n",
    "    top_left_y = int(math.ceil(label.box.center_y - label.box.width / 2))\n",
    "    bottom_right_x = int(math.ceil(label.box.center_x + label.box.length / 2))\n",
    "    bottom_right_y = int(math.ceil(label.box.center_y + label.box.width / 2))\n",
    "    cropped = image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "    nb_pixels = cropped.size\n",
    "    nonzero = np.count_nonzero(cropped)\n",
    "    return (nonzero / nb_pixels) > 0.25\n",
    "\n",
    "\n",
    "for data in dataset:\n",
    "    nb_frames += 1\n",
    "    frame = open_dataset.Frame()\n",
    "    frame.ParseFromString(bytearray(data.numpy()))\n",
    "    image = frame.images[0]\n",
    "    image = np.array(tf.io.decode_jpeg(image.image))\n",
    "\n",
    "    if np.mean(image) < 100:\n",
    "        nb_frames_too_dark += 1\n",
    "        continue\n",
    "\n",
    "    (range_images, camera_projections, _, range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
    "    points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
    "    frame,\n",
    "    range_images,\n",
    "    camera_projections,\n",
    "    range_image_top_pose)\n",
    "    points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
    "        frame,\n",
    "        range_images,\n",
    "        camera_projections,\n",
    "        range_image_top_pose,\n",
    "        ri_index=1)\n",
    "    # 3d points in vehicle frame.\n",
    "    points_all = np.concatenate(points, axis=0)\n",
    "    points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
    "    # camera projection corresponding to each point.\n",
    "    cp_points_all = np.concatenate(cp_points, axis=0)\n",
    "    cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)\n",
    "\n",
    "    all_points = np.append(points_all, points_all_ri2, axis=0)\n",
    "    all_points_cp = np.append(cp_points_all, cp_points_all_ri2, axis=0)\n",
    "\n",
    "    front_points = np.array([point for (point, cp) in zip(all_points, all_points_cp) if cp[0] == 1 or cp[3] == 1])\n",
    "    front_cp_points = np.array([cp[:3] if cp[0] == 1 else cp[3:] for cp in all_points_cp if cp[0] == 1 or cp[3] == 1])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(front_points)\n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold=0.1, ransac_n=4, num_iterations=1000)\n",
    "\n",
    "    front_cp_points_no_road = np.delete(front_cp_points, inliers, axis=0)\n",
    "\n",
    "    alpha_shape = alphashape.alphashape(front_cp_points_no_road[:,1:3], 0.02)\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "\n",
    "    try:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in alpha_shape]\n",
    "    except:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in [alpha_shape]]\n",
    "\n",
    "    mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "    cv2.fillPoly(mask, exteriors, 1)\n",
    "    image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    labels = [camera_labels.labels for camera_labels in frame.camera_labels if camera_labels.name == 1][0]\n",
    "    for label in labels:\n",
    "        if label.box.length < 50 or label.box.width < 50:\n",
    "            nb_labels_too_small += 1\n",
    "        elif not label_in_polygon(label, image):\n",
    "            nb_labels_not_in_poly += 1\n",
    "\n",
    "        labels_per_class[label.type] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/lidar tests/training/' + file for file in os.listdir('Data/lidar tests/training')]\n",
    "\n",
    "nb_frames = 0\n",
    "nb_labels = 0\n",
    "nb_frames_too_dark = 0\n",
    "nb_labels_too_small = 0\n",
    "nb_labels_not_in_poly = 0\n",
    "labels_per_class = [0] * 5\n",
    "\n",
    "def label_in_polygon(label, image):\n",
    "    top_left_x = int(math.ceil(label.box.center_x - label.box.length / 2))\n",
    "    top_left_y = int(math.ceil(label.box.center_y - label.box.width / 2))\n",
    "    bottom_right_x = int(math.ceil(label.box.center_x + label.box.length / 2))\n",
    "    bottom_right_y = int(math.ceil(label.box.center_y + label.box.width / 2))\n",
    "    cropped = image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "    nb_pixels = cropped.size\n",
    "    nonzero = np.count_nonzero(cropped)\n",
    "    return (nonzero / nb_pixels) > 0.25\n",
    "\n",
    "\n",
    "for data in dataset:\n",
    "    nb_frames += 1\n",
    "    frame = open_dataset.Frame()\n",
    "    frame.ParseFromString(bytearray(data.numpy()))\n",
    "    image = frame.images[0]\n",
    "    image = np.array(tf.io.decode_jpeg(image.image))\n",
    "\n",
    "    if np.mean(image) < 100:\n",
    "        nb_frames_too_dark += 1\n",
    "        continue\n",
    "\n",
    "    (range_images, camera_projections, _, range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
    "    points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
    "    frame,\n",
    "    range_images,\n",
    "    camera_projections,\n",
    "    range_image_top_pose)\n",
    "    points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
    "        frame,\n",
    "        range_images,\n",
    "        camera_projections,\n",
    "        range_image_top_pose,\n",
    "        ri_index=1)\n",
    "    # 3d points in vehicle frame.\n",
    "    points_all = np.concatenate(points, axis=0)\n",
    "    points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
    "    # camera projection corresponding to each point.\n",
    "    cp_points_all = np.concatenate(cp_points, axis=0)\n",
    "    cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)\n",
    "\n",
    "    all_points = np.append(points_all, points_all_ri2, axis=0)\n",
    "    all_points_cp = np.append(cp_points_all, cp_points_all_ri2, axis=0)\n",
    "\n",
    "    front_points = np.array([point for (point, cp) in zip(all_points, all_points_cp) if cp[0] == 1 or cp[3] == 1])\n",
    "    front_cp_points = np.array([cp[:3] if cp[0] == 1 else cp[3:] for cp in all_points_cp if cp[0] == 1 or cp[3] == 1])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(front_points)\n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold=0.1, ransac_n=4, num_iterations=1000)\n",
    "\n",
    "    front_cp_points_no_road = np.delete(front_cp_points, inliers, axis=0)\n",
    "\n",
    "    alpha_shape = alphashape.alphashape(front_cp_points_no_road[:,1:3], 0.02)\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "\n",
    "    try:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in alpha_shape]\n",
    "    except:\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in [alpha_shape]]\n",
    "\n",
    "    mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "    cv2.fillPoly(mask, exteriors, 1)\n",
    "    image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    labels = [camera_labels.labels for camera_labels in frame.camera_labels if camera_labels.name == 1][0]\n",
    "    for label in labels:\n",
    "        if label.box.length < 50 or label.box.width < 50:\n",
    "            nb_labels_too_small += 1\n",
    "        elif not label_in_polygon(label, image):\n",
    "            nb_labels_not_in_poly += 1\n",
    "\n",
    "        labels_per_class[label.type] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mti830-3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b69184622dfefc4f97e5135018ed6d3c41e22e1b75804b24dc791ffa51790783"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
